{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
    "\n",
    "@st.cache_resource\n",
    "def load_vectorstore():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(repo_id, token):\n",
    "    return HuggingFaceEndpoint(\n",
    "        repo_id=repo_id,\n",
    "        temperature=0.5,\n",
    "        model_kwargs={\"token\": token, \"max_length\": \"512\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt(template):\n",
    "    return PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "def main():\n",
    "    st.title(\"Interactive Chatbot\")\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        st.chat_message(message[\"role\"]).markdown(message[\"content\"])\n",
    "\n",
    "    user_input = st.chat_input(\"Enter your query\")\n",
    "    if user_input:\n",
    "        st.chat_message(\"user\").markdown(user_input)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        custom_prompt_template = \"\"\"\n",
    "        Use the pieces of information provided in the context to answer the user's question.\n",
    "        If you don't know the answer, just say so. Do not make up answers or include anything not from the context.\n",
    "\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "        HUGGINGFACE_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "        try:\n",
    "            vectorstore = load_vectorstore()\n",
    "            qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=load_llm(HUGGINGFACE_REPO_ID, HF_TOKEN),\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                return_source_documents=True,\n",
    "                chain_type_kwargs={\"prompt\": set_custom_prompt(custom_prompt_template)}\n",
    "            )\n",
    "            response = qa_chain.invoke({\"query\": user_input})\n",
    "            result = response[\"result\"]\n",
    "            sources = response[\"source_documents\"]\n",
    "            output = f\"{result}\\n\\nSource Documents:\\n{sources}\"\n",
    "            st.chat_message(\"assistant\").markdown(output)\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": output})\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
